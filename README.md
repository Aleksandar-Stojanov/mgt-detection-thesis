# Machine-Generated Text Detection: Comparative Analysis

**Bachelor's Thesis**  

**Author:** Aleksandar Stojanov

**Institution:** Faculty of Computer Science and Computer Engineering, Ss. Cyril and Methodius University, Skopje

**Mentor:** Prof. Dr. Sonja Gievska

**Year:** 2025

## Abstract

With the advancement of modern language models, machine-generated text (MGT) has become increasingly similar to human-written text, presenting a serious challenge for detecting text origin. This problem is particularly significant in contexts where authenticity and reliability are crucial, such as social media, literature, and educational institutions.

This thesis presents a comparative analysis of different approaches for detecting machine-generated text. The research encompasses methods based on stylometric features with traditional machine learning models, neural networks with self-attention layers on stylometric features, and modern transformer models that work directly with text. Evaluation was conducted on a complex dataset containing human-written texts and texts generated by advanced language models instructed to imitate human writing style.

Results show that traditional stylometric approaches have limited effectiveness under these conditions, while self-attention models better exploit combinations of stylistic features. The highest performance was achieved with transformer models, which demonstrate excellent capability in distinguishing between human and machine-generated text.

## Research Overview

### Approaches Evaluated

1. **Traditional Machine Learning** (Logistic Regression, SVM, Random Forest, Gradient Boosting)
   - Based on 25 stylometric features
   - Results: ~50% accuracy (near-random performance)
   - Training time: < 30 seconds

2. **SFSC Model** (Stylometric Feature-Based Self-Attention Classifier)
   - Neural network with self-attention on stylometric features
   - Results: 95.3% accuracy, 95.2% F1-score
   - Training time: ~28 seconds
   - Interpretable feature importance

3. **Transformer Models** (MiniLM, ELECTRA, BERT-mini)
   - Fine-tuned on full text
   - Best model: MiniLM-L12-H384-uncased
   - Results: 99.2% accuracy, 99.4% F1-score
   - Training time: ~42 minutes

## Dataset

**Source:** [PAN@CLEF 2025 - Voight-Kampff AI Authorship Verification](https://pan.webis.de/clef25/pan25-web/generated-content-analysis.html)

- **Total samples:** 27,296 texts
- **Human-written:** 10,378 texts
- **Machine-generated:** 16,918 texts
- **Genres:** Essays, Fiction, News
- **LLM Models:** GPT-4o, GPT-4.5, LLaMA 3.3, Gemini 2.0, DeepSeek R1

**Challenge:** Machine models were specifically instructed to imitate human author styles, making detection particularly difficult.

## Repository Structure

```
mgt-detection-thesis/
├── README.md
├── Aleksandar Stojanov - Final thesis.pdf           # Full thesis document (Macedonian)
└──  notebooks/
     ├── Traditional ML - Detect MGT.ipynb           # Traditional ML approach
     ├── SFSC - Detecting MGT.ipynb                  # SFSC model
     └── Transformer models - Detecting MGT.ipynb    # Transformer fine-tuning

```

## Key Findings

### Performance Comparison

| Approach | Accuracy | F1-Score | Training Time | Strengths |
|----------|----------|----------|---------------|-----------|
| Traditional ML | 49.9% | 50.0% | < 30s | Fast, interpretable |
| SFSC (NN + Attention) | 95.3% | 95.2% | ~28s | Balanced, interpretable |
| Transformer (MiniLM) | 99.2% | 99.4% | ~42 min | Highest accuracy |

### Important Stylometric Features (SFSC)

According to self-attention weights, the most important features are:
1. Noun ratio
2. Bigram uniqueness
3. Comma count
4. Readability score
5. Digit ratio
6. Punctuation ratio

## About the Experiments

### 1. Stylometric Analysis & Traditional ML

**What it does:**
- Extracts 25 stylometric features from text
- Visualizes feature distributions
- Trains Logistic Regression, SVM, Random Forest, Gradient Boosting
- Shows why traditional ML fails on modern MGT

**Expected output:** ~50% accuracy (near-random)

### 2. Neural Network with Self-Attention (SFSC)

**What it does:**
- Uses same 25 stylometric features
- Trains neural network with self-attention layer
- Visualizes feature importance
- Shows significant improvement over traditional ML

**Expected output:** ~95% accuracy in < 30 seconds

### 3. Transformer Models

**What it does:**
- Compares lightweight transformers (MiniLM, ELECTRA, BERT-mini)
- Fine-tunes best model (MiniLM) for 3 epochs
- Evaluates against zero-shot detectors
- Generates confusion matrix and ROC curve

**Expected output:** ~99% accuracy in ~42 minutes

## Dataset Access

Due to competition rules, the dataset is not included. To reproduce:

1. Register for [PAN@CLEF 2025](https://pan.webis.de/clef25/pan25-web/generated-content-analysis.html)
2. Download training and validation sets
3. Place `train.jsonl` and `val.jsonl` in notebook directories


## Related Research

This thesis builds upon and compares with:

1. **StyloAI** - Stylometric analysis with Random Forest
   - Opara et al., 2024 ([arXiv](https://arxiv.org/abs/2405.10129))

2. **SFSC on Weibo** - Neural network with self-attention on stylometric features
   - Li et al., 2025 ([CCL](https://aclanthology.org/anthology-files/pdf/ccl/2025.ccl-1.64.pdf))

3. **M4 Dataset** - Multi-generator, multi-domain, multi-lingual detection
   - Wang et al., 2024 ([EACL](https://aclanthology.org/2024.eacl-long.83/))

## Key Conclusions

1. **Modern LLMs defeat traditional stylometry** - Simple stylometric features no longer work when models imitate human style
2. **Self-attention on features helps** - Neural networks can learn complex feature interactions (95% accuracy)
3. **Transformers are most effective** - Direct text processing achieves near-perfect detection (99% accuracy)
4. **Zero-shot detectors fail** - Commercial detectors show poor generalization (<87% accuracy)
5. **Trade-offs exist** - Performance vs. computational cost vs. interpretability

## Challenges & Limitations

- Limited to single dataset (PAN@CLEF 2025)
- Tested only lightweight transformers (not GPT-scale models)
- Cross-domain and cross-generator robustness not extensively tested
- Interpretability decreases with model complexity

## Future Work

- Test on multiple datasets for better generalization
- Explore hybrid approaches (stylometric + transformer features)
- Analyze robustness against newer LLM generations
- Develop methods for cross-domain detection
- Real-world deployment and adversarial testing

## Acknowledgments

Special thanks to:
- Prof. Dr. Sonja Gievska for guidance and mentorship
- PAN@CLEF 2025 organizers for providing the challenging dataset
- Faculty of Computer Science and Computer Engineering, UKIM

---

**Note:** Thesis document is written in Macedonian. For English summary, see this README.
